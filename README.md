---
title: EcoEval-LLM
emoji: ğŸŒ±
colorFrom: green
colorTo: blue
sdk: gradio
sdk_version: 4.0.0
app_file: app.py
pinned: true
---

# ğŸŒ± EcoEval-LLM: Energy & Carbon Benchmarking for LLM Code Generation

**EcoEval-LLM** is a lightweight, reproducible framework for evaluating code-generation models across:

- âœ… **Correctness** (unit-test pass rate)  
- â± **Runtime**  
- âš¡ **Energy consumption (kWh)**  
- ğŸŒ **COâ‚‚ emissions (kg)** via [CodeCarbon](https://github.com/mlco2/codecarbon)

The app runs a small benchmark of Python programming tasks, executes model-generated code, and measures its environmental footprint.

ğŸ”— **Try the live Hugging Face Space:**  
ğŸ‘‰ https://huggingface.co/spaces/singhalamaan116/EcoEval-LLM

---

## ğŸš€ How It Works

1. **You choose:**
   - A Hugging Face model (e.g., `Salesforce/codegen-350M-mono`)
   - A Python benchmark dataset (e.g., `tiny-python-benchmark`)

2. **EcoEval-LLM automatically:**
   - Loads the model using `transformers`
   - Generates code for benchmark prompts
   - Executes and unit-tests the generated solutions
   - Tracks energy + COâ‚‚ using `CodeCarbon.EmissionsTracker`

3. **You get:**
   - **Run-level summary:** accuracy, runtime, energy, COâ‚‚, energy per task, COâ‚‚ per passed task  
   - **Per-task results:** pass/fail and execution latency  
   - **Persistent leaderboard:** stored in `runs.csv` across Space sessions

---

## ğŸ–¥ Run Locally

```bash
git clone <this-repo-url>
cd EcoEval-LLM
pip install -r requirements.txt
python app.py
